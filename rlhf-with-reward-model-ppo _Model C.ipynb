{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-30T07:56:44.488626Z",
     "iopub.status.busy": "2026-01-30T07:56:44.488373Z",
     "iopub.status.idle": "2026-01-30T07:56:46.385236Z",
     "shell.execute_reply": "2026-01-30T07:56:46.384384Z",
     "shell.execute_reply.started": "2026-01-30T07:56:44.488601Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/ultramedical-model-sft-3b/other/default/1/Model_SFT_3B/adapter_model.safetensors\n",
      "/kaggle/input/ultramedical-model-sft-3b/other/default/1/Model_SFT_3B/training_args.bin\n",
      "/kaggle/input/ultramedical-model-sft-3b/other/default/1/Model_SFT_3B/adapter_config.json\n",
      "/kaggle/input/ultramedical-model-sft-3b/other/default/1/Model_SFT_3B/README.md\n",
      "/kaggle/input/ultramedical-model-sft-3b/other/default/1/Model_SFT_3B/tokenizer.json\n",
      "/kaggle/input/ultramedical-model-sft-3b/other/default/1/Model_SFT_3B/tokenizer_config.json\n",
      "/kaggle/input/ultramedical-model-sft-3b/other/default/1/Model_SFT_3B/chat_template.jinja\n",
      "/kaggle/input/ultramedical-model-sft-3b/other/default/1/Model_SFT_3B/special_tokens_map.json\n",
      "/kaggle/input/reward-model-3b/other/default/1/Reward_Model_3B/adapter_model.safetensors\n",
      "/kaggle/input/reward-model-3b/other/default/1/Reward_Model_3B/adapter_config.json\n",
      "/kaggle/input/reward-model-3b/other/default/1/Reward_Model_3B/README.md\n",
      "/kaggle/input/reward-model-3b/other/default/1/Reward_Model_3B/tokenizer.json\n",
      "/kaggle/input/reward-model-3b/other/default/1/Reward_Model_3B/tokenizer_config.json\n",
      "/kaggle/input/reward-model-3b/other/default/1/Reward_Model_3B/chat_template.jinja\n",
      "/kaggle/input/reward-model-3b/other/default/1/Reward_Model_3B/special_tokens_map.json\n",
      "/kaggle/input/ultramedical-preference/ultramedical_preference.json\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T15:31:46.016118Z",
     "iopub.status.busy": "2026-01-23T15:31:46.015736Z",
     "iopub.status.idle": "2026-01-23T15:31:50.223151Z",
     "shell.execute_reply": "2026-01-23T15:31:50.221667Z",
     "shell.execute_reply.started": "2026-01-23T15:31:46.016093Z"
    }
   },
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T07:56:46.387123Z",
     "iopub.status.busy": "2026-01-30T07:56:46.386733Z",
     "iopub.status.idle": "2026-01-30T07:56:54.803302Z",
     "shell.execute_reply": "2026-01-30T07:56:54.802578Z",
     "shell.execute_reply.started": "2026-01-30T07:56:46.387099Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m532.9/532.9 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m149.0/149.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets accelerate peft bitsandbytes trl torch ijson huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T07:56:58.420104Z",
     "iopub.status.busy": "2026-01-30T07:56:58.419390Z",
     "iopub.status.idle": "2026-01-30T07:56:58.852535Z",
     "shell.execute_reply": "2026-01-30T07:56:58.851793Z",
     "shell.execute_reply.started": "2026-01-30T07:56:58.420065Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffa9b5a2145a4409aaaa0212ca26c26d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T07:57:05.822000Z",
     "iopub.status.busy": "2026-01-30T07:57:05.821243Z",
     "iopub.status.idle": "2026-01-30T07:57:05.842479Z",
     "shell.execute_reply": "2026-01-30T07:57:05.841818Z",
     "shell.execute_reply.started": "2026-01-30T07:57:05.821967Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt_id': 'WikiInstruct,8304', 'label_type': 'length', 'prompt': 'Investigate the intricacies of immunometabolism, a distinct subfield of immunology that examines the interconnection between cellular metabolic processes and the functional attributes of immune cells. Clarify the mechanisms by which this symbiosis modulates the comprehensive immune response, and analyze its integration within the broader spectrum of immunological studies, with an emphasis on the implications for metabolic diseases and the consequential effects on the proficiency of the immune system.', 'chosen': [{'content': 'Investigate the intricacies of immunometabolism, a distinct subfield of immunology that examines the interconnection between cellular metabolic processes and the functional attributes of immune cells. Clarify the mechanisms by which this symbiosis modulates the comprehensive immune response, and analyze its integration within the broader spectrum of immunological studies, with an emphasis on the implications for metabolic diseases and the consequential effects on the proficiency of the immune system.', 'role': 'user'}, {'content': ' Immunometabolism is an emerging field of study that explores the intersection between cellular metabolic processes and the functional attributes of immune cells. This subfield has gained significant attention due to the increasing recognition that immune cell functions are heavily influenced by their metabolic programs. The metabolic pathways within immune cells are not merely passive support systems but instead actively modulate and dictate the immune responses. In this investigation, we will delve into the intricacies of immunometabolism, elucidating the mechanisms by which this symbiosis shapes the comprehensive immune response and its implications for metabolic diseases and the proficiency of the immune system.\\n\\nTo begin, we must understand that immune cells, like all other cells in the body, rely on various metabolic processes to generate energy and synthesize essential biomolecules. These metabolic pathways include glycolysis, tricarboxylic acid (TCA) cycle, fatty acid oxidation (FAO), and amino acid metabolism. The differential engagement of these pathways, in turn, influences the functional phenotype of immune cells.\\n\\nOne of the most well-studied examples of immunometabolic crosstalk is the shift between oxidative phosphorylation (OXPHOS) and glycolysis in activated T cells. Upon T cell receptor (TCR) stimulation, na√Øve T cells undergo a metabolic transition from OXPHOS, which is characterized by efficient energy production and biosynthetic quiescence, to aerobic glycolysis, a less efficient energy-generating process that supports the biosynthetic demands required for T cell proliferation and effector functions. This metabolic reprogramming is driven by the PI3K-Akt-mTOR signaling axis, which promotes glycolytic gene expression and enzyme activity. Conversely, the transcription factor FoxO1, which is inhibited upon T cell activation, promotes OXPHOS and fatty acid oxidation, thus maintaining the quiescent state of na√Øve T cells.\\n\\nIn addition to T cells, other immune cells, such as macrophages and dendritic cells (DCs), also exhibit metabolic reprogramming in response to stimuli. Classically activated M1 macrophages, which are involved in the clearance of intracellular pathogens and tumor cells, primarily utilize glycolysis for energy production. In contrast, alternatively activated M2 macrophages, which are associated with tissue repair and immune regulation, predominantly rely on OXPHOS and FAO. Similarly, DCs undergo metabolic reprogramming upon maturation, transitioning from OXPHOS to glycolysis to support their antigen-presenting functions.\\n\\nThe integration of immunometabolism within the broader spectrum of immunological studies has far-reaching implications for various physiological and pathological conditions, particularly metabolic diseases. Obesity, for example, is characterized by chronic low-grade inflammation, which is partly driven by the infiltration of immune cells into adipose tissue. In this setting, adipose tissue-resident macrophages exhibit a metabolic shift towards glycolysis, which promotes the production of proinflammatory cytokines and contributes to insulin resistance. Moreover, obesity impairs T cell metabolism and function, leading to a skewed T cell profile that further exacerbates inflammation and metabolic dysregulation.\\n\\nFurthermore, immunometabolic dysregulation has been implicated in the pathogenesis of other metabolic disorders, such as type 2 diabetes and non-alcoholic fatty liver disease (NAFLD). In type 2 diabetes, impaired insulin signaling in immune cells leads to aberrant metabolic reprogramming and dysfunction, contributing to the perpetuation of the disease. In NAFLD, the accumulation of lipids in hepatic cells activates immune cells, leading to the production of proinflammatory cytokines and the progression of liver damage.\\n\\nUnderstanding the intricate interplay between metabolism and immunity provides novel therapeutic targets for the treatment of metabolic diseases. For instance, modulating immune cell metabolism has been shown to ameliorate insulin resistance and inflammation in obese mice. Additionally, targeting the glycolytic pathway in macrophages has been demonstrated to improve glucose tolerance and insulin sensitivity, highlighting the potential of immunometabolic interventions in the management of metabolic disorders.\\n\\nIn conclusion, immunometabolism is a burgeoning field that elucidates the complex interconnections between cellular metabolic processes and the functional attributes of immune cells. By examining the mechanisms by which this symbiosis modulates the comprehensive immune response, we can better understand the implications of metabolic dysregulation in various physiological and pathological conditions, particularly metabolic diseases. As our knowledge of immunometabolism continues to expand, so too will our ability to harness its potential for the development of innovative therapeutic strategies in the treatment of immune-mediated disorders.', 'role': 'assistant'}], 'rejected': [{'content': 'Investigate the intricacies of immunometabolism, a distinct subfield of immunology that examines the interconnection between cellular metabolic processes and the functional attributes of immune cells. Clarify the mechanisms by which this symbiosis modulates the comprehensive immune response, and analyze its integration within the broader spectrum of immunological studies, with an emphasis on the implications for metabolic diseases and the consequential effects on the proficiency of the immune system.', 'role': 'user'}, {'content': 'Immunometabolism is an emerging field that explores how metabolic processes influence the function and fate of immune cells, and vice versa, how immune responses can shape cellular metabolism. This field has been gaining attention due to its implications in a variety of diseases, including infections, cancer, autoimmunity, and metabolic disorders.\\n\\n**Mechanisms of Immunometabolism:**\\n\\n1. **Metabolic Reprogramming of Immune Cells:**\\n   - Upon activation, immune cells such as T cells, B cells, macrophages, and dendritic cells undergo metabolic reprogramming. For instance, activated T cells switch from oxidative phosphorylation to aerobic glycolysis (the Warburg effect), which provides the necessary building blocks for rapid proliferation and effector function.\\n   - Macrophages also undergo metabolic changes, with M1 (pro-inflammatory) macrophages relying on glycolysis and M2 (anti-inflammatory) macrophages favoring oxidative phosphorylation and fatty acid oxidation.\\n\\n2. **Metabolic Sensors and Immune Regulation:**\\n   - Immune cells express metabolic sensors such as AMP-activated protein kinase (AMPK), mTOR, and sirtuins that integrate cellular energy status with immune function. For example, mTOR signaling promotes the differentiation of T cells into effector subsets, while AMPK activation can have anti-inflammatory effects.\\n\\n3. **Metabolites as Signaling Molecules:**\\n   - Metabolites such as ATP, ADP, NAD+, and reactive oxygen species (ROS) can act as signaling molecules, influencing immune cell function. For example, ROS can act as second messengers that modulate signaling pathways involved in the activation and function of immune cells.\\n\\n4. **Hypoxia and Immune Cell Function:**\\n   - The tumor microenvironment, for instance, is often hypoxic, which can lead to the stabilization of hypoxia-inducible factors (HIFs) and subsequent changes in metabolism and function of both tumor and immune cells. Hypoxia can promote an immunosuppressive microenvironment that aids tumor progression.\\n\\n**Integration within Immunological Studies:**\\n\\nImmunometabolism is becoming increasingly integrated into broader immunological studies. It provides a more nuanced understanding of how immune cells respond to infections, how they are modulated during chronic inflammation, and how they can be harnessed for therapy in diseases such as cancer and autoimmune disorders.\\n\\n**Implications for Metabolic Diseases:**\\n\\n1. **Obesity and Type 2 Diabetes:**\\n   - Metabolic diseases like obesity and type 2 diabetes can lead to chronic low-grade inflammation. Adipose tissue, particularly when it is in an inflamed state, can secrete pro-inflammatory cytokines and chemokines that can affect immune cell metabolism and function.\\n\\n2. **Metabolic Syndrome:**\\n   - Components of metabolic syndrome, such as insulin resistance and dyslipidemia, can influence immune cell function, potentially affecting the risk and progression of cardiovascular diseases.\\n\\n3. **Therapeutic Targets:**\\n   - Understanding the metabolic requirements of different immune cell subsets offers potential therapeutic targets. For example, inhibiting glycolysis or altering fatty acid metabolism could shift the balance between pro-inflammatory and anti-inflammatory macrophages in disease states.\\n\\n**Consequential Effects on Immune System Proficiency:**\\n\\nThe metabolic state of an immune cell can profoundly influence its ability to respond to pathogens and its susceptibility to chronic inflammation or autoimmunity. Dysregulated immune cell metabolism can lead to either an excessive or insufficient immune response, both of which can have detrimental effects on health.\\n\\nIn summary, immunometabolism is a critical area of research that bridges the gap between metabolism and the immune system. It provides a framework for understanding how metabolic pathways regulate immune cell function and how immune responses can impact metabolic health. As research in this field progresses, it holds promise for the development of novel therapeutic strategies for a variety of diseases, including those with a strong metabolic component.', 'role': 'assistant'}], 'metadata': {'golden_answer': None, 'chosen': {'model': 'Mixtral-8x7B-Instruct', 'score': Decimal('5.0'), 'rank': 1, 'evaluation': 'This response is exceptionally detailed and well-structured, providing an in-depth exploration of the mechanisms of immunometabolism and its implications. It effectively integrates immunometabolism within the broader context of immunological studies and discusses its impact on metabolic diseases extensively.'}, 'rejected': {'model': 'Llama-3-8B-UltraMedical', 'score': Decimal('4.0'), 'rank': 2, 'evaluation': 'The response provides a comprehensive overview of immunometabolism, detailing the mechanisms and implications for metabolic diseases. It effectively addresses the scientific inquiry with appropriate terminology and clinical relevance, demonstrating a strong grasp of the subject. However, it could improve by incorporating more examples of how these mechanisms are studied or their specific impacts on disease outcomes.'}}, 'feedback': \"Assistant A provides a detailed and comprehensive response that thoroughly explains the concept of immunometabolism, the mechanisms involved, and its implications in various diseases, particularly metabolic disorders. The answer includes specific examples of how different immune cells undergo metabolic reprogramming and the signaling pathways involved. It also discusses the potential therapeutic applications of targeting immune cell metabolism in treating metabolic diseases. The response is well-structured, uses technical terminology appropriately, and includes evidence-based information, making it highly informative and relevant to the question.\\n\\nAssistant B also provides a clear and well-organized response that covers the basics of immunometabolism and its implications in diseases. It discusses the metabolic reprogramming of immune cells, the role of metabolic sensors, and the effects of metabolites as signaling molecules. The response is accurate and touches on the integration of immunometabolism within broader immunological studies and its therapeutic potential. However, it lacks the depth and detailed examples found in Assistant A's response, making it slightly less thorough in comparison.\\n\\nOverall, Assistant A's response is more detailed and provides a deeper understanding of the mechanisms by which immunometabolism modulates the immune response and its integration within the broader spectrum of immunological studies. It also more effectively addresses the implications for metabolic diseases and the consequential effects on the proficiency of the immune system, adhering closely to the user's instructions.\\n\\nFinal Verdict: [[A]]\"}\n"
     ]
    }
   ],
   "source": [
    "import ijson\n",
    "\n",
    "with open(\"/kaggle/input/ultramedical-preference/ultramedical_preference.json\", \"rb\") as f:\n",
    "    first_item = next(ijson.items(f, \"item\"))\n",
    "    print(first_item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T07:57:06.013177Z",
     "iopub.status.busy": "2026-01-30T07:57:06.012496Z",
     "iopub.status.idle": "2026-01-30T07:57:49.384414Z",
     "shell.execute_reply": "2026-01-30T07:57:49.383812Z",
     "shell.execute_reply.started": "2026-01-30T07:57:06.013149Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-30 07:57:26.781348: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1769759847.115366      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1769759847.211977      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1769759848.094653      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769759848.094697      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769759848.094700      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769759848.094703      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import (\n",
    "    RewardTrainer,\n",
    "    RewardConfig,\n",
    "    PPOTrainer,\n",
    "    PPOConfig,\n",
    "    AutoModelForCausalLMWithValueHead\n",
    ")\n",
    "from transformers import DataCollatorWithPadding\n",
    "import numpy as np\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T07:57:49.385917Z",
     "iopub.status.busy": "2026-01-30T07:57:49.385678Z",
     "iopub.status.idle": "2026-01-30T07:57:49.390459Z",
     "shell.execute_reply": "2026-01-30T07:57:49.389824Z",
     "shell.execute_reply.started": "2026-01-30T07:57:49.385894Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All packages imported successfully!\n",
      "PyTorch version: 2.8.0+cu126\n",
      "CUDA available: True\n",
      "GPU: Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "print(\"‚úÖ All packages imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T07:57:49.391586Z",
     "iopub.status.busy": "2026-01-30T07:57:49.391335Z",
     "iopub.status.idle": "2026-01-30T07:57:49.413064Z",
     "shell.execute_reply": "2026-01-30T07:57:49.412482Z",
     "shell.execute_reply.started": "2026-01-30T07:57:49.391555Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Model Configuration\n",
    "BASE_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "MODEL_SFT_PATH = \"/kaggle/input/ultramedical-model-sft-3b/other/default/1/Model_SFT_3B\"  \n",
    "#REWARD_MODEL_PATH = \"/kaggle/working/reward_model\"\n",
    "REWARD_MODEL_PATH = \"/kaggle/input/reward-model-3b/other/default/1/Reward_Model_3B\"\n",
    "MODEL_C_PATH = \"./kaggle/working/model_c_rlhf_ppo\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T07:57:49.414700Z",
     "iopub.status.busy": "2026-01-30T07:57:49.414451Z",
     "iopub.status.idle": "2026-01-30T07:57:49.428738Z",
     "shell.execute_reply": "2026-01-30T07:57:49.428238Z",
     "shell.execute_reply.started": "2026-01-30T07:57:49.414671Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Data Configuration\n",
    "PREFERENCE_DATA_PATH = \"/kaggle/input/ultramedical-preference/ultramedical_preference.json\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T07:57:49.429683Z",
     "iopub.status.busy": "2026-01-30T07:57:49.429498Z",
     "iopub.status.idle": "2026-01-30T07:57:49.445817Z",
     "shell.execute_reply": "2026-01-30T07:57:49.445013Z",
     "shell.execute_reply.started": "2026-01-30T07:57:49.429664Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration set!\n"
     ]
    }
   ],
   "source": [
    "# Training Configuration\n",
    "REWARD_TRAINING_CONFIG = {\n",
    "    \"output_dir\": REWARD_MODEL_PATH,\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"per_device_train_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 16,\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"logging_steps\": 10,\n",
    "    \"save_steps\": 100,\n",
    "    \"eval_steps\": 100,\n",
    "    \"save_total_limit\": 2,\n",
    "    \"bf16\": True,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"remove_unused_columns\": False,\n",
    "}\n",
    "\n",
    "PPO_CONFIG = {\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"batch_size\": 16,\n",
    "    \"mini_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 16,\n",
    "}\n",
    "\n",
    "# LoRA Configuration\n",
    "from peft import TaskType\n",
    "\n",
    "LORA_CONFIG = {\n",
    "    \"r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"bias\": \"none\",\n",
    "    \"task_type\": TaskType.SEQ_CLS,\n",
    "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "}\n",
    "\n",
    "# Quantization Configuration\n",
    "BNB_CONFIG = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Configuration set!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Preference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T07:57:49.447106Z",
     "iopub.status.busy": "2026-01-30T07:57:49.446802Z",
     "iopub.status.idle": "2026-01-30T07:57:49.459381Z",
     "shell.execute_reply": "2026-01-30T07:57:49.458862Z",
     "shell.execute_reply.started": "2026-01-30T07:57:49.447086Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_preference_data(\n",
    "    file_path: str,\n",
    "    num_samples: int = 1000,\n",
    "    seed: int = 42\n",
    ") -> Dataset:\n",
    "    \"\"\"\n",
    "    Load and preprocess preference data from JSON file\n",
    "    \n",
    "    Expected format:\n",
    "    {\n",
    "        \"prompt\": \"...\",\n",
    "        \"chosen\": [{\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}],\n",
    "        \"rejected\": [{\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Loading preference data...\")\n",
    "\n",
    "    # Load JSON\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Normalize structure\n",
    "    if isinstance(data, dict):\n",
    "        if \"data\" in data:\n",
    "            data = data[\"data\"]\n",
    "        elif \"rows\" in data:\n",
    "            data = data[\"rows\"]\n",
    "\n",
    "    print(f\"Total available preference pairs: {len(data)}\")\n",
    "\n",
    "    # üîπ Random sampling\n",
    "    random.seed(seed)\n",
    "    if len(data) > num_samples:\n",
    "        data = random.sample(data, num_samples)\n",
    "        print(f\"Randomly sampled {num_samples} examples\")\n",
    "    else:\n",
    "        print(\"Dataset smaller than requested sample size ‚Äî using all data\")\n",
    "\n",
    "    processed_data = []\n",
    "\n",
    "    for idx, example in enumerate(data):\n",
    "        try:\n",
    "            # Prompt\n",
    "            prompt = example.get(\"prompt\")\n",
    "            if not prompt and \"chosen\" in example and isinstance(example[\"chosen\"], list):\n",
    "                prompt = example[\"chosen\"][0].get(\"content\", \"\")\n",
    "\n",
    "            if not prompt:\n",
    "                continue\n",
    "\n",
    "            # Chosen\n",
    "            if isinstance(example.get(\"chosen\"), list):\n",
    "                chosen = next(\n",
    "                    (t[\"content\"] for t in example[\"chosen\"] if t.get(\"role\") == \"assistant\"),\n",
    "                    None\n",
    "                )\n",
    "            else:\n",
    "                chosen = example.get(\"chosen\")\n",
    "\n",
    "            # Rejected\n",
    "            if isinstance(example.get(\"rejected\"), list):\n",
    "                rejected = next(\n",
    "                    (t[\"content\"] for t in example[\"rejected\"] if t.get(\"role\") == \"assistant\"),\n",
    "                    None\n",
    "                )\n",
    "            else:\n",
    "                rejected = example.get(\"rejected\")\n",
    "\n",
    "            if chosen and rejected:\n",
    "                processed_data.append({\n",
    "                    \"prompt\": prompt,\n",
    "                    \"chosen\": chosen,\n",
    "                    \"rejected\": rejected\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Skipped example {idx}: {e}\")\n",
    "\n",
    "    print(f\"Successfully processed {len(processed_data)} examples\")\n",
    "\n",
    "    dataset = Dataset.from_pandas(pd.DataFrame(processed_data))\n",
    "\n",
    "    # Preview\n",
    "    print(\"\\nüìä Sample:\")\n",
    "    print(\"Prompt:\", dataset[0][\"prompt\"][:100], \"...\")\n",
    "    print(\"Chosen:\", dataset[0][\"chosen\"][:100], \"...\")\n",
    "    print(\"Rejected:\", dataset[0][\"rejected\"][:100], \"...\")\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T07:57:49.460439Z",
     "iopub.status.busy": "2026-01-30T07:57:49.460196Z",
     "iopub.status.idle": "2026-01-30T07:57:59.659890Z",
     "shell.execute_reply": "2026-01-30T07:57:59.659162Z",
     "shell.execute_reply.started": "2026-01-30T07:57:49.460405Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preference data...\n",
      "Total available preference pairs: 109353\n",
      "Randomly sampled 1000 examples\n",
      "Successfully processed 1000 examples\n",
      "\n",
      "üìä Sample:\n",
      "Prompt: Ive been recently having more nose bleeds lately. 4-5 times per day to be exact. And I have the wors ...\n",
      "Chosen: \n",
      "\n",
      "I'm so sorry to hear that you're experiencing frequent nosebleeds and a persistent cough. While it ...\n",
      "Rejected: I'm not a doctor, but I can provide you with some general information. Frequent nosebleeds accompani ...\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "preference_dataset = load_preference_data(PREFERENCE_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T07:57:59.661209Z",
     "iopub.status.busy": "2026-01-30T07:57:59.660897Z",
     "iopub.status.idle": "2026-01-30T07:57:59.682021Z",
     "shell.execute_reply": "2026-01-30T07:57:59.681494Z",
     "shell.execute_reply.started": "2026-01-30T07:57:59.661186Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Dataset prepared!\n",
      "   Training samples: 900\n",
      "   Evaluation samples: 100\n"
     ]
    }
   ],
   "source": [
    "# Split into train/eval\n",
    "split_dataset = preference_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = split_dataset['train']\n",
    "eval_dataset = split_dataset['test']\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset prepared!\")\n",
    "print(f\"   Training samples: {len(train_dataset)}\")\n",
    "print(f\"   Evaluation samples: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization for Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T07:57:59.682973Z",
     "iopub.status.busy": "2026-01-30T07:57:59.682713Z",
     "iopub.status.idle": "2026-01-30T07:58:01.764041Z",
     "shell.execute_reply": "2026-01-30T07:58:01.763300Z",
     "shell.execute_reply.started": "2026-01-30T07:57:59.682924Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a238aa1edf76463f959788b3c54208c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b22f739d87d448da944245c2d991246e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7122135e1454b248fb965ac72694c71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizer loaded successfully\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "print(\"‚úÖ Tokenizer loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T07:58:01.766278Z",
     "iopub.status.busy": "2026-01-30T07:58:01.766027Z",
     "iopub.status.idle": "2026-01-30T07:58:01.845902Z",
     "shell.execute_reply": "2026-01-30T07:58:01.845008Z",
     "shell.execute_reply.started": "2026-01-30T07:58:01.766255Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting dataset for reward model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "176bd10f2b7e428daf8fc4ba3856cc29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a677162b74c04c4d88e556c93de67fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Formatting complete!\n",
      "Sample chosen: <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "A 25 yr old female presented with malaise and generalized weakness since 6 months. Her appetite is reduced and she has giddiness and palpitat...\n",
      "Sample rejected: <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "A 25 yr old female presented with malaise and generalized weakness since 6 months. Her appetite is reduced and she has giddiness and palpitat...\n"
     ]
    }
   ],
   "source": [
    "def format_for_reward_model(examples: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Format examples for reward model training\n",
    "    TRL RewardTrainer expects 'chosen' and 'rejected' as full text strings\n",
    "    \"\"\"\n",
    "    \n",
    "    prompts = examples['prompt']\n",
    "    chosen = examples['chosen']\n",
    "    rejected = examples['rejected']\n",
    "    \n",
    "    # Format in Llama chat format\n",
    "    def format_text(prompt, response):\n",
    "        text = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|>\"\n",
    "        text += f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n{response}<|eot_id|>\"\n",
    "        return text\n",
    "    \n",
    "    # Create full text for chosen and rejected\n",
    "    chosen_texts = [format_text(p, c) for p, c in zip(prompts, chosen)]\n",
    "    rejected_texts = [format_text(p, r) for p, r in zip(prompts, rejected)]\n",
    "    \n",
    "    return {\n",
    "        'chosen': chosen_texts,\n",
    "        'rejected': rejected_texts,\n",
    "    }\n",
    "\n",
    "# Apply formatting (keep as text, don't tokenize yet)\n",
    "print(\"Formatting dataset for reward model...\")\n",
    "formatted_train = train_dataset.map(\n",
    "    format_for_reward_model,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "formatted_eval = eval_dataset.map(\n",
    "    format_for_reward_model,\n",
    "    batched=True,\n",
    "    remove_columns=eval_dataset.column_names\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Formatting complete!\")\n",
    "print(f\"Sample chosen: {formatted_train[0]['chosen'][:200]}...\")\n",
    "print(f\"Sample rejected: {formatted_train[0]['rejected'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T07:58:01.847101Z",
     "iopub.status.busy": "2026-01-30T07:58:01.846817Z",
     "iopub.status.idle": "2026-01-30T07:58:01.851472Z",
     "shell.execute_reply": "2026-01-30T07:58:01.850737Z",
     "shell.execute_reply.started": "2026-01-30T07:58:01.847079Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print(\"Loading tokenizer...\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_side = \"right\"\n",
    "\n",
    "# def format_for_reward_model(examples: Dict) -> Dict:\n",
    "#     \"\"\"\n",
    "#     Format examples for reward model training\n",
    "    \n",
    "#     Returns:\n",
    "#         Dict with 'input_ids_chosen', 'attention_mask_chosen',\n",
    "#         'input_ids_rejected', 'attention_mask_rejected'\n",
    "#     \"\"\"\n",
    "    \n",
    "#     prompts = examples['prompt']\n",
    "#     chosen = examples['chosen']\n",
    "#     rejected = examples['rejected']\n",
    "    \n",
    "#     # Format in Llama chat format\n",
    "#     def format_text(prompt, response):\n",
    "#         text = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|>\"\n",
    "#         text += f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n{response}<|eot_id|>\"\n",
    "#         return text\n",
    "    \n",
    "#     # Tokenize chosen\n",
    "#     chosen_texts = [format_text(p, c) for p, c in zip(prompts, chosen)]\n",
    "#     chosen_tokens = tokenizer(\n",
    "#         chosen_texts,\n",
    "#         truncation=True,\n",
    "#         max_length=1024,\n",
    "#         padding=\"max_length\",\n",
    "#         return_tensors=\"pt\"\n",
    "#     )\n",
    "    \n",
    "#     # Tokenize rejected\n",
    "#     rejected_texts = [format_text(p, r) for p, r in zip(prompts, rejected)]\n",
    "#     rejected_tokens = tokenizer(\n",
    "#         rejected_texts,\n",
    "#         truncation=True,\n",
    "#         max_length=1024,\n",
    "#         padding=\"max_length\",\n",
    "#         return_tensors=\"pt\"\n",
    "#     )\n",
    "    \n",
    "#     return {\n",
    "#         'input_ids_chosen': chosen_tokens['input_ids'],\n",
    "#         'attention_mask_chosen': chosen_tokens['attention_mask'],\n",
    "#         'input_ids_rejected': rejected_tokens['input_ids'],\n",
    "#         'attention_mask_rejected': rejected_tokens['attention_mask'],\n",
    "#     }\n",
    "\n",
    "# # Apply tokenization\n",
    "# print(\"Tokenizing dataset for reward model...\")\n",
    "# tokenized_train = train_dataset.map(\n",
    "#     format_for_reward_model,\n",
    "#     batched=True,\n",
    "#     remove_columns=train_dataset.column_names\n",
    "# )\n",
    "\n",
    "# tokenized_eval = eval_dataset.map(\n",
    "#     format_for_reward_model,\n",
    "#     batched=True,\n",
    "#     remove_columns=eval_dataset.column_names\n",
    "# )\n",
    "\n",
    "# print(\"‚úÖ Tokenization complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T08:49:03.577644Z",
     "iopub.status.busy": "2026-01-27T08:49:03.577347Z",
     "iopub.status.idle": "2026-01-27T10:01:32.422669Z",
     "shell.execute_reply": "2026-01-27T10:01:32.421969Z",
     "shell.execute_reply.started": "2026-01-27T08:49:03.577620Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 1: TRAINING REWARD MODEL\n",
      "================================================================================\n",
      "Loading base model for reward modeling...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "919fc8511cc6459997586829a85e3f8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d167eef96ed64d489eba5efef119efbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4053ac9b2774945b67a5b19a2fd4282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d98a1a8078401fae30f873df3018ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d1600f85e464232b0d2d392b3b52fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "211e5ab53cfa4baa9a7ad19d119659ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-3B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 9,178,112 / 1,812,644,864 (0.51%)\n",
      "\n",
      "üöÄ Starting manual reward model training...\n",
      "Training on device: cuda:0\n",
      "\n",
      "üìä Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/900 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Training:   1%|          | 10/900 [00:42<1:15:45,  5.11s/it, loss=0.4047, lr=0.00e+00]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 10: loss=0.8779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|‚ñè         | 20/900 [01:30<1:06:43,  4.55s/it, loss=0.5407, lr=1.11e-07]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 20: loss=1.1751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|‚ñé         | 30/900 [02:12<52:23,  3.61s/it, loss=0.4579, lr=1.11e-07]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 30: loss=1.0345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|‚ñç         | 40/900 [03:09<1:22:52,  5.78s/it, loss=1.3897, lr=2.22e-07]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 40: loss=1.1420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|‚ñå         | 50/900 [03:56<1:14:52,  5.29s/it, loss=5.2806, lr=3.33e-07]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 50: loss=1.2487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|‚ñã         | 60/900 [04:32<49:39,  3.55s/it, loss=0.1818, lr=3.33e-07]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 60: loss=1.1783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|‚ñä         | 70/900 [05:15<1:02:13,  4.50s/it, loss=1.2400, lr=4.44e-07]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 70: loss=1.2432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|‚ñâ         | 80/900 [05:59<57:33,  4.21s/it, loss=0.8469, lr=5.56e-07]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 80: loss=1.1973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|‚ñà         | 90/900 [06:40<53:31,  3.96s/it, loss=1.9324, lr=5.56e-07]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 90: loss=1.1831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|‚ñà         | 100/900 [07:21<51:53,  3.89s/it, loss=0.2010, lr=6.67e-07] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 100: loss=1.1630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|‚ñà‚ñè        | 110/900 [08:13<1:00:08,  4.57s/it, loss=0.1686, lr=6.67e-07]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 110: loss=1.1125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|‚ñà‚ñé        | 120/900 [09:06<1:12:11,  5.55s/it, loss=0.9657, lr=7.78e-07]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 120: loss=1.1037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|‚ñà‚ñç        | 130/900 [09:51<59:15,  4.62s/it, loss=0.7562, lr=8.89e-07]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 130: loss=1.0853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|‚ñà‚ñå        | 140/900 [10:39<56:58,  4.50s/it, loss=1.0449, lr=8.89e-07]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 140: loss=1.0685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|‚ñà‚ñã        | 150/900 [11:29<1:06:58,  5.36s/it, loss=0.5670, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 150: loss=1.1554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|‚ñà‚ñä        | 160/900 [12:19<1:02:42,  5.08s/it, loss=0.1903, lr=1.11e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 160: loss=1.1649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|‚ñà‚ñâ        | 170/900 [13:02<58:03,  4.77s/it, loss=0.1074, lr=1.11e-06]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 170: loss=1.1689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|‚ñà‚ñà        | 180/900 [13:47<58:52,  4.91s/it, loss=0.9728, lr=1.22e-06]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 180: loss=1.1452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|‚ñà‚ñà        | 190/900 [14:28<46:51,  3.96s/it, loss=0.6161, lr=1.22e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 190: loss=1.1734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|‚ñà‚ñà‚ñè       | 200/900 [15:11<51:15,  4.39s/it, loss=0.1560, lr=1.33e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 200: loss=1.1812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|‚ñà‚ñà‚ñé       | 210/900 [16:00<55:21,  4.81s/it, loss=0.3375, lr=1.44e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 210: loss=1.1839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|‚ñà‚ñà‚ñç       | 220/900 [16:48<50:15,  4.43s/it, loss=0.5501, lr=1.44e-06]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 220: loss=1.1762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|‚ñà‚ñà‚ñå       | 230/900 [17:29<42:00,  3.76s/it, loss=0.6112, lr=1.56e-06]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 230: loss=1.1539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|‚ñà‚ñà‚ñã       | 240/900 [18:13<40:39,  3.70s/it, loss=0.6527, lr=1.67e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 240: loss=1.1498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|‚ñà‚ñà‚ñä       | 250/900 [18:58<46:51,  4.32s/it, loss=0.6019, lr=1.67e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 250: loss=1.1479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|‚ñà‚ñà‚ñâ       | 260/900 [19:42<51:47,  4.86s/it, loss=1.1948, lr=1.78e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 260: loss=1.1339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|‚ñà‚ñà‚ñà       | 270/900 [20:23<51:47,  4.93s/it, loss=0.9793, lr=1.78e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 270: loss=1.1309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|‚ñà‚ñà‚ñà       | 280/900 [21:06<41:53,  4.05s/it, loss=0.0599, lr=1.89e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 280: loss=1.1529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|‚ñà‚ñà‚ñà‚ñè      | 290/900 [22:01<1:01:48,  6.08s/it, loss=0.9786, lr=2.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 290: loss=1.1672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|‚ñà‚ñà‚ñà‚ñé      | 300/900 [22:48<44:52,  4.49s/it, loss=1.2331, lr=2.00e-06]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 300: loss=1.1447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|‚ñà‚ñà‚ñà‚ñç      | 310/900 [23:36<50:01,  5.09s/it, loss=0.8622, lr=2.11e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 310: loss=1.1352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|‚ñà‚ñà‚ñà‚ñå      | 320/900 [24:16<34:38,  3.58s/it, loss=1.0815, lr=2.22e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 320: loss=1.1353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|‚ñà‚ñà‚ñà‚ñã      | 330/900 [25:04<39:22,  4.14s/it, loss=0.1034, lr=2.22e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 330: loss=1.1273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|‚ñà‚ñà‚ñà‚ñä      | 340/900 [25:47<40:04,  4.29s/it, loss=0.0474, lr=2.33e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 340: loss=1.1130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|‚ñà‚ñà‚ñà‚ñâ      | 350/900 [26:39<43:07,  4.70s/it, loss=0.6616, lr=2.33e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 350: loss=1.1020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|‚ñà‚ñà‚ñà‚ñà      | 360/900 [27:21<33:37,  3.74s/it, loss=2.0564, lr=2.44e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 360: loss=1.1029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|‚ñà‚ñà‚ñà‚ñà      | 370/900 [28:11<42:32,  4.82s/it, loss=1.1222, lr=2.56e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 370: loss=1.0957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 380/900 [28:54<39:30,  4.56s/it, loss=2.3671, lr=2.56e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 380: loss=1.1009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 390/900 [29:38<38:11,  4.49s/it, loss=0.1613, lr=2.67e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 390: loss=1.0909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 400/900 [30:23<33:48,  4.06s/it, loss=0.1268, lr=2.78e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 400: loss=1.0886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 410/900 [31:06<33:59,  4.16s/it, loss=0.2943, lr=2.78e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 410: loss=1.0948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 420/900 [31:54<39:05,  4.89s/it, loss=1.9006, lr=2.89e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 420: loss=1.1008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 430/900 [32:30<33:22,  4.26s/it, loss=0.2418, lr=2.89e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 430: loss=1.1004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 440/900 [33:12<32:33,  4.25s/it, loss=2.5771, lr=3.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 440: loss=1.0998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 450/900 [34:06<38:13,  5.10s/it, loss=0.9383, lr=3.11e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 450: loss=1.0951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 460/900 [34:55<39:02,  5.32s/it, loss=0.9112, lr=3.11e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 460: loss=1.0959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 470/900 [35:48<39:20,  5.49s/it, loss=2.8639, lr=3.22e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 470: loss=1.0912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 480/900 [36:34<32:06,  4.59s/it, loss=0.4267, lr=3.33e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 480: loss=1.0930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 490/900 [37:22<31:56,  4.68s/it, loss=1.5475, lr=3.33e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 490: loss=1.0865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 500/900 [38:17<37:17,  5.59s/it, loss=1.0510, lr=3.44e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 500: loss=1.0857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 510/900 [39:08<35:47,  5.51s/it, loss=0.5116, lr=3.44e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 510: loss=1.0769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 520/900 [39:50<25:47,  4.07s/it, loss=1.0586, lr=3.56e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 520: loss=1.0775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 530/900 [40:42<35:38,  5.78s/it, loss=1.7207, lr=3.67e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 530: loss=1.0770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 540/900 [41:24<26:40,  4.45s/it, loss=3.9435, lr=3.67e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 540: loss=1.0765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 550/900 [42:09<24:42,  4.24s/it, loss=0.5534, lr=3.78e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 550: loss=1.0695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 560/900 [42:54<22:11,  3.92s/it, loss=0.5775, lr=3.89e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 560: loss=1.0670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 570/900 [43:32<16:16,  2.96s/it, loss=0.8373, lr=3.89e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 570: loss=1.0678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 580/900 [44:20<24:41,  4.63s/it, loss=1.4610, lr=4.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 580: loss=1.0637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 590/900 [45:07<23:12,  4.49s/it, loss=0.2397, lr=4.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 590: loss=1.0577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 600/900 [46:01<28:35,  5.72s/it, loss=0.7882, lr=4.11e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 600: loss=1.0555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 610/900 [46:45<24:05,  4.99s/it, loss=0.7248, lr=4.22e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 610: loss=1.0603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 620/900 [47:26<19:29,  4.18s/it, loss=0.6839, lr=4.22e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 620: loss=1.0627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 630/900 [48:17<22:21,  4.97s/it, loss=0.1131, lr=4.33e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 630: loss=1.0565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 640/900 [49:00<18:12,  4.20s/it, loss=0.9429, lr=4.44e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 640: loss=1.0501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 650/900 [49:37<14:49,  3.56s/it, loss=0.3352, lr=4.44e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 650: loss=1.0442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 660/900 [50:20<15:14,  3.81s/it, loss=1.4079, lr=4.56e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 660: loss=1.0446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 670/900 [51:07<17:00,  4.44s/it, loss=1.9976, lr=4.56e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 670: loss=1.0567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 680/900 [51:47<16:37,  4.53s/it, loss=2.3627, lr=4.67e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 680: loss=1.0533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 690/900 [52:32<15:53,  4.54s/it, loss=1.0648, lr=4.78e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 690: loss=1.0494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 700/900 [53:16<14:21,  4.31s/it, loss=0.3610, lr=4.78e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 700: loss=1.0448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 710/900 [54:00<13:33,  4.28s/it, loss=0.8183, lr=4.89e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 710: loss=1.0429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 720/900 [54:49<15:32,  5.18s/it, loss=0.3856, lr=5.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 720: loss=1.0403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 730/900 [55:34<13:35,  4.80s/it, loss=1.1916, lr=5.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 730: loss=1.0412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 740/900 [56:20<12:56,  4.85s/it, loss=1.9310, lr=5.11e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 740: loss=1.0486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 750/900 [57:05<11:29,  4.60s/it, loss=0.4832, lr=5.11e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 750: loss=1.0483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 760/900 [57:50<09:39,  4.14s/it, loss=0.8328, lr=5.22e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 760: loss=1.0476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 770/900 [58:43<09:53,  4.56s/it, loss=1.8080, lr=5.33e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 770: loss=1.0442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 780/900 [59:33<10:03,  5.03s/it, loss=0.4390, lr=5.33e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 780: loss=1.0408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 790/900 [1:00:22<07:38,  4.17s/it, loss=0.4263, lr=5.44e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 790: loss=1.0376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 800/900 [1:01:21<10:06,  6.07s/it, loss=1.0093, lr=5.56e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 800: loss=1.0417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 810/900 [1:02:08<06:59,  4.67s/it, loss=0.9939, lr=5.56e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 810: loss=1.0395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 820/900 [1:02:43<04:21,  3.27s/it, loss=0.5114, lr=5.67e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 820: loss=1.0436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 830/900 [1:03:29<05:47,  4.96s/it, loss=0.3735, lr=5.67e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 830: loss=1.0452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 840/900 [1:04:17<05:20,  5.34s/it, loss=0.3143, lr=5.78e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 840: loss=1.0426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 850/900 [1:04:58<03:49,  4.58s/it, loss=1.0070, lr=5.89e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 850: loss=1.0418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 860/900 [1:05:46<03:23,  5.10s/it, loss=0.7635, lr=5.89e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 860: loss=1.0397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 870/900 [1:06:21<01:43,  3.46s/it, loss=0.7348, lr=6.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 870: loss=1.0371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 880/900 [1:07:17<01:49,  5.48s/it, loss=0.6216, lr=6.11e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 880: loss=1.0369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 890/900 [1:08:07<00:52,  5.27s/it, loss=1.4784, lr=6.11e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 890: loss=1.0357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 900/900 [1:08:55<00:00,  4.60s/it, loss=0.7497, lr=6.22e-06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 900: loss=1.0328\n",
      "\n",
      "‚úÖ Epoch 1 Complete - Average Loss: 1.0328\n",
      "\n",
      "üìä Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [02:39<00:00,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Evaluation Loss: 0.8977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: TRAINING REWARD MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load base model for reward modeling\n",
    "print(\"Loading base model for reward modeling...\")\n",
    "reward_base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    num_labels=1,  # Regression for reward score\n",
    "    quantization_config=BNB_CONFIG,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Prepare for training\n",
    "reward_base_model = prepare_model_for_kbit_training(reward_base_model)\n",
    "\n",
    "# Add LoRA\n",
    "lora_config = LoraConfig(**LORA_CONFIG)\n",
    "reward_model = get_peft_model(reward_base_model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in reward_model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in reward_model.parameters())\n",
    "print(f\"Trainable: {trainable_params:,} / {total_params:,} ({100*trainable_params/total_params:.2f}%)\")\n",
    "\n",
    "# Manual Training Loop (More Reliable)\n",
    "print(\"\\nüöÄ Starting manual reward model training...\")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create dataloader\n",
    "train_dataloader = DataLoader(\n",
    "    formatted_train,\n",
    "    batch_size=REWARD_TRAINING_CONFIG[\"per_device_train_batch_size\"],\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    formatted_eval,\n",
    "    batch_size=REWARD_TRAINING_CONFIG[\"per_device_train_batch_size\"],\n",
    "    shuffle=False,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# Optimizer and scheduler\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    reward_model.parameters(),\n",
    "    lr=REWARD_TRAINING_CONFIG[\"learning_rate\"]\n",
    ")\n",
    "\n",
    "num_training_steps = len(train_dataloader) * REWARD_TRAINING_CONFIG[\"num_train_epochs\"]\n",
    "num_warmup_steps = int(num_training_steps * REWARD_TRAINING_CONFIG[\"warmup_ratio\"])\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "reward_model.train()\n",
    "device = reward_model.device if hasattr(reward_model, 'device') else next(reward_model.parameters()).device\n",
    "print(f\"Training on device: {device}\")\n",
    "\n",
    "gradient_accumulation_steps = REWARD_TRAINING_CONFIG[\"gradient_accumulation_steps\"]\n",
    "\n",
    "for epoch in range(REWARD_TRAINING_CONFIG[\"num_train_epochs\"]):\n",
    "    print(f\"\\nüìä Epoch {epoch + 1}/{REWARD_TRAINING_CONFIG['num_train_epochs']}\")\n",
    "    \n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Training\")\n",
    "    \n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        # Tokenize chosen\n",
    "        chosen_inputs = tokenizer(\n",
    "            batch['chosen'],\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=1024\n",
    "        )\n",
    "        \n",
    "        # Tokenize rejected\n",
    "        rejected_inputs = tokenizer(\n",
    "            batch['rejected'],\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=1024\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        chosen_inputs = {k: v.to(device) for k, v in chosen_inputs.items()}\n",
    "        rejected_inputs = {k: v.to(device) for k, v in rejected_inputs.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        chosen_rewards = reward_model(**chosen_inputs).logits\n",
    "        rejected_rewards = reward_model(**rejected_inputs).logits\n",
    "        \n",
    "        # Loss: chosen should have higher reward\n",
    "        # Using ranking loss: -log(sigmoid(chosen - rejected))\n",
    "        loss = -torch.nn.functional.logsigmoid(chosen_rewards - rejected_rewards).mean()\n",
    "        \n",
    "        # Scale loss for gradient accumulation\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Update weights every N steps\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(reward_model.parameters(), 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item() * gradient_accumulation_steps:.4f}',\n",
    "            'lr': f'{scheduler.get_last_lr()[0]:.2e}'\n",
    "        })\n",
    "        \n",
    "        # Logging\n",
    "        if (step + 1) % REWARD_TRAINING_CONFIG[\"logging_steps\"] == 0:\n",
    "            avg_loss = (total_loss * gradient_accumulation_steps) / (step + 1)\n",
    "            print(f\"  Step {step + 1}: loss={avg_loss:.4f}\")\n",
    "    \n",
    "    # Epoch summary\n",
    "    avg_loss = (total_loss * gradient_accumulation_steps) / len(train_dataloader)\n",
    "    print(f\"\\n‚úÖ Epoch {epoch + 1} Complete - Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Evaluation\n",
    "    if eval_dataloader and len(eval_dataloader) > 0:\n",
    "        print(f\"\\nüìä Running evaluation...\")\n",
    "        reward_model.eval()\n",
    "        \n",
    "        eval_loss = 0\n",
    "        eval_steps = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "                chosen_inputs = tokenizer(\n",
    "                    batch['chosen'],\n",
    "                    return_tensors='pt',\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=1024\n",
    "                )\n",
    "                \n",
    "                rejected_inputs = tokenizer(\n",
    "                    batch['rejected'],\n",
    "                    return_tensors='pt',\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=1024\n",
    "                )\n",
    "                \n",
    "                chosen_inputs = {k: v.to(device) for k, v in chosen_inputs.items()}\n",
    "                rejected_inputs = {k: v.to(device) for k, v in rejected_inputs.items()}\n",
    "                \n",
    "                chosen_rewards = reward_model(**chosen_inputs).logits\n",
    "                rejected_rewards = reward_model(**rejected_inputs).logits\n",
    "                \n",
    "                loss = -torch.nn.functional.logsigmoid(chosen_rewards - rejected_rewards).mean()\n",
    "                \n",
    "                eval_loss += loss.item()\n",
    "                eval_steps += 1\n",
    "        \n",
    "        avg_eval_loss = eval_loss / eval_steps\n",
    "        print(f\"‚úÖ Evaluation Loss: {avg_eval_loss:.4f}\")\n",
    "        \n",
    "        reward_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T10:04:11.236347Z",
     "iopub.status.busy": "2026-01-27T10:04:11.235994Z",
     "iopub.status.idle": "2026-01-27T10:04:11.963642Z",
     "shell.execute_reply": "2026-01-27T10:04:11.962834Z",
     "shell.execute_reply.started": "2026-01-27T10:04:11.236317Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving reward model to /kaggle/working/reward_model_final...\n",
      "‚úÖ Reward model training complete!\n"
     ]
    }
   ],
   "source": [
    "# Save reward model\n",
    "REWARD_MODEL_PATH= \"/kaggle/working/reward_model_final\"\n",
    "\n",
    "print(f\"üíæ Saving reward model to {REWARD_MODEL_PATH}...\")\n",
    "reward_model.save_pretrained(REWARD_MODEL_PATH)\n",
    "tokenizer.save_pretrained(REWARD_MODEL_PATH)\n",
    "\n",
    "print(\"‚úÖ Reward model training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T10:04:58.855541Z",
     "iopub.status.busy": "2026-01-27T10:04:58.855247Z",
     "iopub.status.idle": "2026-01-27T10:05:01.096608Z",
     "shell.execute_reply": "2026-01-27T10:05:01.095917Z",
     "shell.execute_reply.started": "2026-01-27T10:04:58.855514Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Zipped reward model saved to /kaggle/working/reward_model_final.zip\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "REWARD_MODEL_PATH = \"/kaggle/working/reward_model_final\"\n",
    "ZIP_PATH = \"/kaggle/working/reward_model_final.zip\"\n",
    "\n",
    "# Remove zip if it already exists\n",
    "if os.path.exists(ZIP_PATH):\n",
    "    os.remove(ZIP_PATH)\n",
    "\n",
    "# Create zip\n",
    "shutil.make_archive(\n",
    "    base_name=ZIP_PATH.replace(\".zip\", \"\"),\n",
    "    format=\"zip\",\n",
    "    root_dir=REWARD_MODEL_PATH\n",
    ")\n",
    "\n",
    "print(f\"üì¶ Zipped reward model saved to {ZIP_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model_SFT for PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T07:58:01.852532Z",
     "iopub.status.busy": "2026-01-30T07:58:01.852260Z",
     "iopub.status.idle": "2026-01-30T07:58:52.547304Z",
     "shell.execute_reply": "2026-01-30T07:58:52.546610Z",
     "shell.execute_reply.started": "2026-01-30T07:58:01.852500Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 2: PREPARING FOR PPO TRAINING\n",
      "================================================================================\n",
      "Loading Model_SFT...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca7c6eb859d49458ae1bca2bc273429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab9fad03652c470a811e8a0e52649807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8a0838ba64b4a26914eab0102dadb2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f412d160973f437c9a7058b164b890b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4106b0fbc38642bab3cd86339de58940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2a1f98744844340b9f6610d49a76604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb4afb5a15b4437896d0b6119ca0fbf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The `device_map` argument is not provided. We will override the device_map argument. to set the entire model on the current device. If you want to set the model on multiple devices, please provide a custom `device_map` argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model_SFT loaded and wrapped for PPO!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/trl/experimental/ppo/modeling_value_head.py:273: FutureWarning: The `AutoModelForCausalLMWithValueHead` is now located in `trl.experimental`. Please update your imports to `from trl.experimental.ppo import AutoModelForCausalLMWithValueHead`. The current import path will be removed and no longer supported in TRL 0.29. For more information, see https://github.com/huggingface/trl/issues/4223.\n",
      "  model = cls(pretrained_model, **multi_adapter_args, **trl_model_args)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: PREPARING FOR PPO TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load Model_SFT (your baseline from Step 1)\n",
    "print(\"Loading Model_SFT...\")\n",
    "base_model_ppo = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=BNB_CONFIG,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load LoRA adapter (Model_SFT)\n",
    "model_sft = PeftModel.from_pretrained(base_model_ppo, MODEL_SFT_PATH)\n",
    "\n",
    "# Wrap model for PPO (adds value head)\n",
    "ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(model_sft)\n",
    "\n",
    "print(\"‚úÖ Model_SFT loaded and wrapped for PPO!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Reward Model for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T07:58:52.549071Z",
     "iopub.status.busy": "2026-01-30T07:58:52.548236Z",
     "iopub.status.idle": "2026-01-30T07:58:59.549776Z",
     "shell.execute_reply": "2026-01-30T07:58:59.549008Z",
     "shell.execute_reply.started": "2026-01-30T07:58:52.549033Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained reward model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faba95aa327f49139a193d86dfc42437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-3B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Reward model loaded for inference!\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading trained reward model...\")\n",
    "\n",
    "# Load the trained reward model\n",
    "reward_model_inference = AutoModelForSequenceClassification.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    num_labels=1,\n",
    "    quantization_config=BNB_CONFIG,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load LoRA weights\n",
    "reward_model_inference = PeftModel.from_pretrained(\n",
    "    reward_model_inference,\n",
    "    REWARD_MODEL_PATH\n",
    ")\n",
    "reward_model_inference.eval()\n",
    "\n",
    "print(\"‚úÖ Reward model loaded for inference!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare PPO Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T07:58:59.551032Z",
     "iopub.status.busy": "2026-01-30T07:58:59.550721Z",
     "iopub.status.idle": "2026-01-30T07:58:59.637129Z",
     "shell.execute_reply": "2026-01-30T07:58:59.636066Z",
     "shell.execute_reply.started": "2026-01-30T07:58:59.550996Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35637cf54c3f4843a97e64cb6b3c298f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PPO dataset prepared: 900 prompts\n",
      "Sample query: <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "A 25 yr old female presented with malaise...\n"
     ]
    }
   ],
   "source": [
    "def prepare_ppo_dataset(dataset: Dataset) -> Dataset:\n",
    "    \"\"\"Prepare dataset for PPO training (only prompts needed)\"\"\"\n",
    "    \n",
    "    def format_prompt(example):\n",
    "        prompt = example['prompt']\n",
    "        formatted = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        return {'query': formatted}\n",
    "    \n",
    "    return dataset.map(format_prompt, remove_columns=dataset.column_names)\n",
    "\n",
    "ppo_train_dataset = prepare_ppo_dataset(train_dataset)\n",
    "\n",
    "print(f\"‚úÖ PPO dataset prepared: {len(ppo_train_dataset)} prompts\")\n",
    "print(f\"Sample query: {ppo_train_dataset[0]['query'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T07:59:15.703330Z",
     "iopub.status.busy": "2026-01-30T07:59:15.702925Z",
     "iopub.status.idle": "2026-01-30T08:02:15.955992Z",
     "shell.execute_reply": "2026-01-30T08:02:15.954895Z",
     "shell.execute_reply.started": "2026-01-30T07:59:15.703305Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 3: PPO TRAINING (Experimental API)\n",
      "================================================================================\n",
      "Creating value model...\n",
      "‚úÖ Value model created on CPU\n",
      "Creating frozen reference model...\n",
      "‚úÖ Reference model created on CPU\n",
      "‚úÖ Reward model on CPU\n",
      "\n",
      "Creating PPO trainer...\n",
      "‚úÖ PPO trainer created!\n",
      "\n",
      "üöÄ Starting PPO training using trainer.train()...\n",
      "Note: The trainer will handle the training loop internally\n",
      "\n",
      "===training policy===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_55/1929145176.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;31m# Use the built-in train() method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;31m# This should handle generation, reward computation, and PPO updates automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0mppo_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n‚úÖ Training completed successfully!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trl/experimental/ppo/ppo_trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    645\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0;31m# backward compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mon_train_begin\u001b[0;34m(self, args, state, control)\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_training_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_train_begin\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mcall_event\u001b[0;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m             result = getattr(callback, event)(\n\u001b[0m\u001b[1;32m    557\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/integrations/integration_utils.py\u001b[0m in \u001b[0;36mon_train_begin\u001b[0;34m(self, args, state, control, model, **kwargs)\u001b[0m\n\u001b[1;32m    964\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessing_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/integrations/integration_utils.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, args, state, model, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m                 self._wandb.init(\n\u001b[0m\u001b[1;32m    894\u001b[0m                     \u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"WANDB_PROJECT\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"huggingface\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0minit_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(entity, project, dir, id, name, notes, tags, config, config_exclude_keys, config_include_keys, allow_val_change, group, job_type, mode, force, anonymous, reinit, resume, resume_from, fork_from, save_code, tensorboard, sync_tensorboard, monitor_gym, settings)\u001b[0m\n\u001b[1;32m   1521\u001b[0m         \u001b[0mwi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_WandbInit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_telemetry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1523\u001b[0;31m         \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_login\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_settings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1524\u001b[0m         \u001b[0mrun_settings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_warnings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_run_settings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_settings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36mmaybe_login\u001b[0;34m(self, init_settings)\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         wandb_login._login(\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0manonymous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manonymous\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36m_login\u001b[0;34m(anonymous, key, relogin, host, force, timeout, verify, referrer, update_api_key, _silent, _disable_warning)\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mkey_is_pre_configured\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_status\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwlogin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreferrer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreferrer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36mprompt_api_key\u001b[0;34m(self, referrer)\u001b[0m\n\u001b[1;32m    236\u001b[0m     ) -> Tuple[Optional[str], ApiKeyStatus]:\n\u001b[1;32m    237\u001b[0m         \u001b[0;34m\"\"\"Updates the global API key by prompting the user.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreferrer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mApiKeyStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOTTY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             directive = (\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36m_prompt_api_key\u001b[0;34m(self, referrer)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m                 key = apikey.prompt_api_key(\n\u001b[0m\u001b[1;32m    215\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_settings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                     \u001b[0mapi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wandb/sdk/lib/apikey.py\u001b[0m in \u001b[0;36mprompt_api_key\u001b[0;34m(settings, api, input_callback, browser_callback, no_offline, no_create, local, referrer)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mjupyter\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"google.colab\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mlog_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mterm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLOG_STRING_NOCOLOR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjupyter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattempt_colab_login\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapp_url\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mkey\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wandb/jupyter.py\u001b[0m in \u001b[0;36mattempt_colab_login\u001b[0;34m(app_url, referrer)\u001b[0m\n\u001b[1;32m    352\u001b[0m     )\n\u001b[1;32m    353\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_wandbApiKey\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import copy\n",
    "import shutil\n",
    "from typing import List\n",
    "from transformers import GenerationConfig\n",
    "from trl.experimental.ppo import PPOConfig, PPOTrainer\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: PPO TRAINING (Experimental API)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# =========================\n",
    "# PPO CONFIG\n",
    "# =========================\n",
    "ppo_config = PPOConfig(**PPO_CONFIG)\n",
    "\n",
    "# =========================\n",
    "# POLICY MODEL SETUP\n",
    "# =========================\n",
    "policy_model = ppo_model.half().cuda() \n",
    "\n",
    "# Assign generation_config to policy model\n",
    "policy_model.generation_config = GenerationConfig(\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# VALUE MODEL (frozen copy)\n",
    "# =========================\n",
    "print(\"Creating value model...\")\n",
    "value_model = copy.deepcopy(policy_model).eval().cpu() \n",
    "for p in value_model.parameters():\n",
    "    p.requires_grad = False\n",
    "print(\"‚úÖ Value model created on CPU\")\n",
    "\n",
    "# =========================\n",
    "# REFERENCE MODEL (frozen)\n",
    "# =========================\n",
    "print(\"Creating frozen reference model...\")\n",
    "ref_model = copy.deepcopy(policy_model.pretrained_model).eval().cpu() \n",
    "for p in ref_model.parameters():\n",
    "    p.requires_grad = False\n",
    "print(\"‚úÖ Reference model created on CPU\")\n",
    "\n",
    "# =========================\n",
    "# REWARD MODEL ON CPU\n",
    "# =========================\n",
    "reward_model_inference = reward_model_inference.half().cpu()\n",
    "print(\"‚úÖ Reward model on CPU\")\n",
    "\n",
    "# =========================\n",
    "# REWARD FUNCTION WITH CPU OFFLOAD\n",
    "# =========================\n",
    "def get_reward(response_texts: List[str], prompt_texts: List[str]) -> List[float]:\n",
    "    \"\"\"Compute rewards for generated responses\"\"\"\n",
    "    texts = []\n",
    "    for prompt, response in zip(prompt_texts, response_texts):\n",
    "        text = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|>\"\n",
    "        text += f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n{response}<|eot_id|>\"\n",
    "        texts.append(text)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "    reward_model_inference.to(device)\n",
    "    \n",
    "    # Keep input_ids as long type, don't convert to half\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.autocast(\"cuda\"), torch.no_grad():\n",
    "        rewards = reward_model_inference(**inputs).logits.squeeze(-1)\n",
    "\n",
    "    reward_model_inference.cpu()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return rewards.cpu().tolist()\n",
    "\n",
    "# =========================\n",
    "# PPO TRAINER SETUP\n",
    "# =========================\n",
    "print(\"\\nCreating PPO trainer...\")\n",
    "ppo_trainer = PPOTrainer(\n",
    "    ppo_config,\n",
    "    processing_class=tokenizer,\n",
    "    model=policy_model.pretrained_model,\n",
    "    ref_model=ref_model,\n",
    "    reward_model=reward_model_inference,\n",
    "    value_model=value_model.pretrained_model,\n",
    "    train_dataset=ppo_train_dataset,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ PPO trainer created!\")\n",
    "\n",
    "# =========================\n",
    "# OPTION 1: USE BUILT-IN TRAIN() METHOD\n",
    "# =========================\n",
    "print(\"\\nüöÄ Starting PPO training using trainer.train()...\")\n",
    "print(\"Note: The trainer will handle the training loop internally\\n\")\n",
    "\n",
    "try:\n",
    "    # Use the built-in train() method\n",
    "    # This should handle generation, reward computation, and PPO updates automatically\n",
    "    ppo_trainer.train()\n",
    "    \n",
    "    print(\"\\n‚úÖ Training completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è Built-in train() failed: {str(e)}\")\n",
    "    print(\"Falling back to manual training loop...\\n\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # =========================\n",
    "    # OPTION 2: MANUAL TRAINING LOOP\n",
    "    # =========================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Using manual training loop with training_step()\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create dataloader\n",
    "    def collate_fn(batch):\n",
    "        \"\"\"Collate function that tokenizes text queries\"\"\"\n",
    "        query_texts = [item['query'] for item in batch]\n",
    "        tokenized = tokenizer(\n",
    "            query_texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "        )\n",
    "        return tokenized\n",
    "\n",
    "    ppo_dataloader = DataLoader(\n",
    "        ppo_train_dataset,\n",
    "        batch_size=1, \n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    \n",
    "    generation_kwargs = {\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9,\n",
    "        \"do_sample\": True,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    }\n",
    "    \n",
    "    for epoch in range(1):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"üìä EPOCH {epoch+1}/1\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "        epoch_rewards = []\n",
    "        \n",
    "        for batch_idx, batch in enumerate(tqdm(ppo_dataloader, desc=f\"Epoch {epoch+1}\")):\n",
    "            # Prepare inputs\n",
    "            query_tensors = batch[\"input_ids\"].cuda()\n",
    "            attention_mask = batch[\"attention_mask\"].cuda()\n",
    "            \n",
    "            # Generate responses\n",
    "            with torch.autocast(\"cuda\"), torch.no_grad():\n",
    "                response_tensors = policy_model.generate(\n",
    "                    query_tensors,\n",
    "                    attention_mask=attention_mask,\n",
    "                    **generation_kwargs\n",
    "                )\n",
    "\n",
    "            # Decode queries\n",
    "            queries_decoded = []\n",
    "            for q in query_tensors:\n",
    "                decoded = tokenizer.decode(q, skip_special_tokens=True)\n",
    "                if \"user<|end_header_id|>\" in decoded:\n",
    "                    prompt = decoded.split(\"user<|end_header_id|>\")[1].split(\"<|eot_id|>\")[0].strip()\n",
    "                else:\n",
    "                    prompt = decoded\n",
    "                queries_decoded.append(prompt)\n",
    "\n",
    "            # Decode responses\n",
    "            responses_decoded = []\n",
    "            for r in response_tensors:\n",
    "                decoded = tokenizer.decode(r, skip_special_tokens=True)\n",
    "                if \"assistant<|end_header_id|>\" in decoded:\n",
    "                    response = decoded.split(\"assistant<|end_header_id|>\")[-1].split(\"<|eot_id|>\")[0].strip()\n",
    "                else:\n",
    "                    response = decoded\n",
    "                responses_decoded.append(response)\n",
    "\n",
    "            # Get rewards\n",
    "            rewards = get_reward(responses_decoded, queries_decoded)\n",
    "            rewards_tensor = torch.tensor(rewards, device=query_tensors.device, dtype=torch.float16)\n",
    "\n",
    "            # Create batch dict for training_step\n",
    "            batch_dict = {\n",
    "                'input_ids': query_tensors,\n",
    "                'attention_mask': attention_mask,\n",
    "                'response_ids': response_tensors,\n",
    "                'rewards': rewards_tensor,\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                # Use training_step with the batch\n",
    "                loss = ppo_trainer.training_step(policy_model.pretrained_model, batch_dict)\n",
    "                \n",
    "                epoch_rewards.extend(rewards)\n",
    "                \n",
    "                if batch_idx % 10 == 0:\n",
    "                    print(f\"\\n  Batch {batch_idx}: Reward: {np.mean(rewards):.3f}, Loss: {loss:.4f}\")\n",
    "                    if batch_idx == 0:\n",
    "                        print(f\"  Sample prompt: {queries_decoded[0][:100]}...\")\n",
    "                        print(f\"  Sample response: {responses_decoded[0][:100]}...\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ö†Ô∏è Error in batch {batch_idx}: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "\n",
    "            if batch_idx % 50 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        # Print statistics\n",
    "        if epoch_rewards:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"üìà EPOCH {epoch+1} COMPLETE\")\n",
    "            print(f\"   Mean Reward: {np.mean(epoch_rewards):.3f}\")\n",
    "            print(f\"   Std Reward: {np.std(epoch_rewards):.3f}\")\n",
    "            print(f\"   Min Reward: {np.min(epoch_rewards):.3f}\")\n",
    "            print(f\"   Max Reward: {np.max(epoch_rewards):.3f}\")\n",
    "            print(f\"{'='*80}\\n\")\n",
    "\n",
    "# =========================\n",
    "# SAVE CHECKPOINT\n",
    "# =========================\n",
    "checkpoint_path = f\"/kaggle/working/ppo_final\"\n",
    "print(f\"\\nüíæ Saving final checkpoint to {checkpoint_path}...\")\n",
    "policy_model.save_pretrained(checkpoint_path)\n",
    "tokenizer.save_pretrained(checkpoint_path)\n",
    "shutil.make_archive(checkpoint_path, 'zip', checkpoint_path)\n",
    "print(f\"‚úÖ Checkpoint saved and zipped!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ PPO TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T20:13:56.550399Z",
     "iopub.status.busy": "2026-01-29T20:13:56.549545Z",
     "iopub.status.idle": "2026-01-29T20:13:56.563762Z",
     "shell.execute_reply": "2026-01-29T20:13:56.563019Z",
     "shell.execute_reply.started": "2026-01-29T20:13:56.550356Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PPO TRAINER API DIAGNOSTIC\n",
      "================================================================================\n",
      "\n",
      "1. All public methods and attributes:\n",
      "--------------------------------------------------------------------------------\n",
      "  ‚Ä¢ accelerator - attribute\n",
      "  ‚úì add_callback() - method\n",
      "  ‚Ä¢ args - attribute\n",
      "  ‚úì autocast_smart_context_manager() - method\n",
      "  ‚úì call_model_init() - method\n",
      "  ‚Ä¢ callback_handler - attribute\n",
      "  ‚Ä¢ callbacks - attribute\n",
      "  ‚úì compare_trainer_and_checkpoint_args() - method\n",
      "  ‚úì compute_loss() - method\n",
      "  ‚úì compute_loss_context_manager() - method\n",
      "  ‚Ä¢ control - attribute\n",
      "  ‚úì create_accelerator_and_postprocess() - method\n",
      "  ‚úì create_model_card() - method\n",
      "  ‚úì create_optimizer() - method\n",
      "  ‚úì create_optimizer_and_scheduler() - method\n",
      "  ‚úì create_scheduler() - method\n",
      "  ‚Ä¢ current_flos - attribute\n",
      "  ‚úì data_collator() - method\n",
      "  ‚Ä¢ dataloader - attribute\n",
      "  ‚Ä¢ eval_dataloader - attribute\n",
      "  ‚Ä¢ eval_dataset - attribute\n",
      "  ‚úì evaluate() - method\n",
      "  ‚úì evaluation_loop() - method\n",
      "  ‚úì floating_point_ops() - method\n",
      "  ‚úì generate_completions() - method\n",
      "  ‚úì get_batch_samples() - method\n",
      "  ‚úì get_decay_parameter_names() - method\n",
      "  ‚úì get_eval_dataloader() - method\n",
      "  ‚úì get_learning_rates() - method\n",
      "  ‚úì get_num_trainable_parameters() - method\n",
      "  ‚úì get_optimizer_cls_and_kwargs() - method\n",
      "  ‚úì get_optimizer_group() - method\n",
      "  ‚úì get_test_dataloader() - method\n",
      "  ‚úì get_total_train_batch_size() - method\n",
      "  ‚úì get_tp_size() - method\n",
      "  ‚úì get_train_dataloader() - method\n",
      "  ‚Ä¢ hp_search_backend - attribute\n",
      "  ‚Ä¢ hub_model_id - attribute\n",
      "  ‚úì hyperparameter_search() - method\n",
      "  ‚úì init_hf_repo() - method\n",
      "  ‚Ä¢ is_deepspeed_enabled - attribute\n",
      "  ‚Ä¢ is_fsdp_enabled - attribute\n",
      "  ‚úì is_local_process_zero() - method\n",
      "  ‚Ä¢ is_peft_model - attribute\n",
      "  ‚úì is_world_process_zero() - method\n",
      "  ‚Ä¢ local_dataloader_batch_size - attribute\n",
      "  ‚Ä¢ local_seed - attribute\n",
      "  ‚úì log() - method\n",
      "  ‚úì log_metrics() - method\n",
      "  ‚Ä¢ lr_scheduler - attribute\n",
      "  ‚úì metrics_format() - method\n",
      "  ‚úì model() - method\n",
      "  ‚Ä¢ model_adapter_name - attribute\n",
      "  ‚úì null_ref_context() - method\n",
      "  ‚úì num_examples() - method\n",
      "  ‚úì num_tokens() - method\n",
      "  ‚Ä¢ optimizer - attribute\n",
      "  ‚Ä¢ optimizer_cls_and_kwargs - attribute\n",
      "  ‚úì policy_model() - method\n",
      "  ‚úì pop_callback() - method\n",
      "  ‚úì predict() - method\n",
      "  ‚úì prediction_loop() - method\n",
      "  ‚úì prediction_step() - method\n",
      "  ‚úì processing_class() - method\n",
      "  ‚úì propagate_args_to_deepspeed() - method\n",
      "  ‚úì push_to_hub() - method\n",
      "  ‚Ä¢ ref_adapter_name - attribute\n",
      "  ‚úì ref_model() - method\n",
      "  ‚úì remove_callback() - method\n",
      "  ‚úì reward_model() - method\n",
      "  ‚Ä¢ sample_generations_freq - attribute\n",
      "  ‚úì save_metrics() - method\n",
      "  ‚úì save_model() - method\n",
      "  ‚úì save_state() - method\n",
      "  ‚úì set_initial_training_values() - method\n",
      "  ‚Ä¢ state - attribute\n",
      "  ‚Ä¢ stop_token_id - attribute\n",
      "  ‚úì store_flos() - method\n",
      "  ‚úì tokenizer() - method\n",
      "  ‚úì torch_jit_model_eval() - method\n",
      "  ‚úì train() - method\n",
      "  ‚Ä¢ train_dataset - attribute\n",
      "  ‚Ä¢ train_dataset_len - attribute\n",
      "  ‚úì training_step() - method\n",
      "  ‚úì value_model() - method\n",
      "\n",
      "2. Looking for training-related methods:\n",
      "--------------------------------------------------------------------------------\n",
      "  ‚Üí compare_trainer_and_checkpoint_args()\n",
      "  ‚Üí compute_loss()\n",
      "  ‚Üí compute_loss_context_manager()\n",
      "  ‚Üí create_optimizer()\n",
      "  ‚Üí create_optimizer_and_scheduler()\n",
      "  ‚Üí get_batch_samples()\n",
      "  ‚Üí get_num_trainable_parameters()\n",
      "  ‚Üí get_optimizer_cls_and_kwargs()\n",
      "  ‚Üí get_optimizer_group()\n",
      "  ‚Üí get_total_train_batch_size()\n",
      "  ‚Üí get_train_dataloader()\n",
      "  ‚Üí prediction_step()\n",
      "  ‚Üí set_initial_training_values()\n",
      "  ‚Üí train()\n",
      "  ‚Üí training_step()\n",
      "\n",
      "3. Most likely training methods: compare_trainer_and_checkpoint_args, compute_loss, compute_loss_context_manager, create_optimizer, create_optimizer_and_scheduler, get_batch_samples, get_num_trainable_parameters, get_optimizer_cls_and_kwargs, get_optimizer_group, get_total_train_batch_size, get_train_dataloader, prediction_step, set_initial_training_values, train, training_step\n",
      "\n",
      "4. Checking method signatures:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "compare_trainer_and_checkpoint_args(training_args, trainer_state)\n",
      "\n",
      "compute_loss(model: torch.nn.modules.module.Module, inputs: dict[str, typing.Union[torch.Tensor, typing.Any]], return_outputs: bool = False, num_items_in_batch: Optional[torch.Tensor] = None)\n",
      "    How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
      "    \n",
      "    Args:\n",
      "\n",
      "compute_loss_context_manager()\n",
      "    A helper wrapper to group together context managers.\n",
      "\n",
      "create_optimizer()\n",
      "    Setup the optimizer.\n",
      "    \n",
      "    We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n",
      "\n",
      "create_optimizer_and_scheduler(num_training_steps: int)\n",
      "    Setup the optimizer and the learning rate scheduler.\n",
      "    \n",
      "    We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n",
      "\n",
      "================================================================================\n",
      "DIAGNOSTIC COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PPOTrainer API Diagnostic Script\n",
    "Run this to identify the correct method to use with the experimental PPOTrainer\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PPO TRAINER API DIAGNOSTIC\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. All public methods and attributes:\")\n",
    "print(\"-\" * 80)\n",
    "for attr in sorted(dir(ppo_trainer)):\n",
    "    if not attr.startswith('_'):\n",
    "        obj = getattr(ppo_trainer, attr)\n",
    "        if callable(obj):\n",
    "            print(f\"  ‚úì {attr}() - method\")\n",
    "        else:\n",
    "            print(f\"  ‚Ä¢ {attr} - attribute\")\n",
    "\n",
    "print(\"\\n2. Looking for training-related methods:\")\n",
    "print(\"-\" * 80)\n",
    "training_keywords = ['step', 'update', 'train', 'forward', 'batch', 'loss', 'optimize']\n",
    "found_methods = []\n",
    "for attr in dir(ppo_trainer):\n",
    "    if not attr.startswith('_'):\n",
    "        if any(keyword in attr.lower() for keyword in training_keywords):\n",
    "            obj = getattr(ppo_trainer, attr)\n",
    "            if callable(obj):\n",
    "                found_methods.append(attr)\n",
    "                print(f\"  ‚Üí {attr}()\")\n",
    "\n",
    "if found_methods:\n",
    "    print(f\"\\n3. Most likely training methods: {', '.join(found_methods)}\")\n",
    "else:\n",
    "    print(\"\\n3. ‚ö†Ô∏è No obvious training methods found\")\n",
    "\n",
    "print(\"\\n4. Checking method signatures:\")\n",
    "print(\"-\" * 80)\n",
    "import inspect\n",
    "for method_name in found_methods[:5]:  # Check first 5 methods\n",
    "    try:\n",
    "        method = getattr(ppo_trainer, method_name)\n",
    "        sig = inspect.signature(method)\n",
    "        print(f\"\\n{method_name}{sig}\")\n",
    "        # Try to get docstring\n",
    "        if method.__doc__:\n",
    "            doc_lines = method.__doc__.strip().split('\\n')[:3]\n",
    "            for line in doc_lines:\n",
    "                print(f\"    {line.strip()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{method_name}: Could not inspect - {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DIAGNOSTIC COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Final Model C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# SAVE FINAL MODEL\n",
    "# =========================\n",
    "final_model_path = \"/kaggle/working/model_c_ppo_final\"\n",
    "policy_model.save_pretrained(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "shutil.make_archive(final_model_path, 'zip', final_model_path)\n",
    "\n",
    "print(\"\\n‚úÖ PPO TRAINING COMPLETE (1 EPOCH)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING MODEL C\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test prompt\n",
    "test_prompt = \"A 55-year-old man presents with chest pain and shortness of breath. What are the key differential diagnoses?\"\n",
    "\n",
    "formatted_prompt = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{test_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "# Generate\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(ppo_model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = ppo_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"\\nüìù Prompt: {test_prompt}\")\n",
    "print(f\"\\nü§ñ Response:\\n{response}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL DONE! üéâ\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nModel C (RLHF + PPO) is ready at: {MODEL_C_PATH}\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Download Model C from Kaggle\")\n",
    "print(\"2. Compare with Model A, B, and D\")\n",
    "print(\"3. Evaluate on test set\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 9321972,
     "sourceId": 14593696,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 570909,
     "modelInstanceId": 558341,
     "sourceId": 732677,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 573386,
     "modelInstanceId": 560788,
     "sourceId": 735603,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
