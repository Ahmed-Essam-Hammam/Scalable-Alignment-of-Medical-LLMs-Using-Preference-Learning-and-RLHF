{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-26T17:46:07.672304Z",
     "iopub.status.busy": "2026-01-26T17:46:07.671818Z",
     "iopub.status.idle": "2026-01-26T17:46:08.710578Z",
     "shell.execute_reply": "2026-01-26T17:46:08.709848Z",
     "shell.execute_reply.started": "2026-01-26T17:46:07.672268Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/medical-processed/medical_qa_processed.jsonl\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T17:46:19.533763Z",
     "iopub.status.busy": "2026-01-26T17:46:19.533471Z",
     "iopub.status.idle": "2026-01-26T17:46:26.167642Z",
     "shell.execute_reply": "2026-01-26T17:46:26.166960Z",
     "shell.execute_reply.started": "2026-01-26T17:46:19.533739Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.4.2)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Collecting trl\n",
      "  Downloading trl-0.27.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.36.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (26.0rc2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.2.1rc0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.12.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.6.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Downloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.27.1-py3-none-any.whl (532 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m532.9/532.9 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes, trl\n",
      "Successfully installed bitsandbytes-0.49.1 trl-0.27.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets accelerate peft bitsandbytes trl huggingface_hub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T17:46:41.310133Z",
     "iopub.status.busy": "2026-01-26T17:46:41.309825Z",
     "iopub.status.idle": "2026-01-26T17:46:41.677053Z",
     "shell.execute_reply": "2026-01-26T17:46:41.676381Z",
     "shell.execute_reply.started": "2026-01-26T17:46:41.310104Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19a666514a734ee784954280dd1fcd10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T17:46:49.761109Z",
     "iopub.status.busy": "2026-01-26T17:46:49.760430Z",
     "iopub.status.idle": "2026-01-26T17:46:49.764590Z",
     "shell.execute_reply": "2026-01-26T17:46:49.763835Z",
     "shell.execute_reply.started": "2026-01-26T17:46:49.761082Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T17:46:50.013160Z",
     "iopub.status.busy": "2026-01-26T17:46:50.012940Z",
     "iopub.status.idle": "2026-01-26T17:47:21.281847Z",
     "shell.execute_reply": "2026-01-26T17:47:21.281254Z",
     "shell.execute_reply.started": "2026-01-26T17:46:50.013139Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-26 17:47:04.012352: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1769449624.186174      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1769449624.233530      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1769449624.610461      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769449624.610493      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769449624.610496      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769449624.610499      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T17:47:21.283243Z",
     "iopub.status.busy": "2026-01-26T17:47:21.283010Z",
     "iopub.status.idle": "2026-01-26T17:47:21.286415Z",
     "shell.execute_reply": "2026-01-26T17:47:21.285868Z",
     "shell.execute_reply.started": "2026-01-26T17:47:21.283221Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "OUTPUT_DIR = \"/kaggle/working/llama3.2-3b-ultramedical-lora\"\n",
    "DATASET_PATH = \"/kaggle/input/medical-processed/medical_qa_processed.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T17:47:21.287583Z",
     "iopub.status.busy": "2026-01-26T17:47:21.287347Z",
     "iopub.status.idle": "2026-01-26T17:47:21.305716Z",
     "shell.execute_reply": "2026-01-26T17:47:21.305204Z",
     "shell.execute_reply.started": "2026-01-26T17:47:21.287562Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# LoRA configuration\n",
    "\n",
    "LORA_CONFIG = {\n",
    "    \"r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"bias\": \"none\",\n",
    "    \"task_type\": \"CAUSAL_LM\",\n",
    "    \"target_modules\": [\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "# Updated Training configuration for 2x T4\n",
    "TRAINING_CONFIG = {\n",
    "    \"output_dir\": OUTPUT_DIR,\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"per_device_train_batch_size\": 2, \n",
    "    \"gradient_accumulation_steps\": 8,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"optim\": \"paged_adamw_32bit\", # 32bit is often more stable on T4s with 4-bit loading\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"logging_steps\": 10,\n",
    "    \"save_steps\": 2000,\n",
    "    \"save_total_limit\": 2,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"bf16\": True,\n",
    "    \"ddp_find_unused_parameters\": False,           \n",
    "    \"report_to\": \"none\",\n",
    "}\n",
    "\n",
    "#Quantization config for 4-bit loading\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T17:47:21.307467Z",
     "iopub.status.busy": "2026-01-26T17:47:21.307099Z",
     "iopub.status.idle": "2026-01-26T17:47:21.322420Z",
     "shell.execute_reply": "2026-01-26T17:47:21.321820Z",
     "shell.execute_reply.started": "2026-01-26T17:47:21.307442Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_ultramedical_dataset(dataset_path, max_samples=2000, seed=42):\n",
    "    \"\"\"\n",
    "    Load up to `max_samples` random samples from UltraMedical JSONL.\n",
    "    Uses reservoir sampling for memory efficiency.\n",
    "    \"\"\"\n",
    "\n",
    "    random.seed(seed)\n",
    "    reservoir = []\n",
    "    valid_count = 0\n",
    "\n",
    "    with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line_num, line in enumerate(f, start=1):\n",
    "            try:\n",
    "                record = json.loads(line)\n",
    "\n",
    "                if \"conversations\" not in record or len(record[\"conversations\"]) == 0:\n",
    "                    continue\n",
    "\n",
    "                valid_count += 1\n",
    "\n",
    "                # Fill reservoir\n",
    "                if len(reservoir) < max_samples:\n",
    "                    reservoir.append(record)\n",
    "                else:\n",
    "                    # Replace elements with decreasing probability\n",
    "                    j = random.randint(0, valid_count - 1)\n",
    "                    if j < max_samples:\n",
    "                        reservoir[j] = record\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"‚ö†Ô∏è Skipping invalid JSON at line {line_num}\")\n",
    "\n",
    "    df = pd.DataFrame(reservoir)\n",
    "\n",
    "    print(f\"‚úÖ Loaded {len(df)} random samples (from {valid_count} valid records)\")\n",
    "\n",
    "    return Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T17:47:21.323457Z",
     "iopub.status.busy": "2026-01-26T17:47:21.323202Z",
     "iopub.status.idle": "2026-01-26T17:47:21.338800Z",
     "shell.execute_reply": "2026-01-26T17:47:21.338208Z",
     "shell.execute_reply.started": "2026-01-26T17:47:21.323436Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def format_conversation(example):\n",
    "    \"\"\"\n",
    "    Convert UltraMedical format to Llama 3.1 chat format\n",
    "\n",
    "    UltraMedical format:\n",
    "    [\n",
    "        {\"from\": \"human\", \"value\": \"question\"},\n",
    "        {\"from\": \"gpt\", \"value\": \"answer\"}\n",
    "    ]\n",
    "\n",
    "    Llama 3.1 format:\n",
    "    <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "    question<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    answer<|eot_id|>\n",
    "    \"\"\"\n",
    "\n",
    "    formatted_text = \"<|begin_of_text|>\"\n",
    "\n",
    "    for turn in example[\"conversations\"]:\n",
    "        if turn[\"from\"] == \"human\":\n",
    "            role = \"user\"\n",
    "        elif turn[\"from\"] == \"gpt\":\n",
    "            role = \"assistant\"\n",
    "        else:\n",
    "            continue  # skip unknown roles\n",
    "\n",
    "        content = turn[\"value\"].strip()\n",
    "\n",
    "        formatted_text += (\n",
    "            f\"<|start_header_id|>{role}<|end_header_id|>\\n\\n\"\n",
    "            f\"{content}<|eot_id|>\"\n",
    "        )\n",
    "\n",
    "    return {\"text\": formatted_text}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T17:47:21.339944Z",
     "iopub.status.busy": "2026-01-26T17:47:21.339594Z",
     "iopub.status.idle": "2026-01-26T17:47:37.572547Z",
     "shell.execute_reply": "2026-01-26T17:47:37.571789Z",
     "shell.execute_reply.started": "2026-01-26T17:47:21.339912Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "‚úÖ Loaded 2000 random samples (from 409262 valid records)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_ultramedical_dataset(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T17:47:37.573775Z",
     "iopub.status.busy": "2026-01-26T17:47:37.573509Z",
     "iopub.status.idle": "2026-01-26T17:47:37.743531Z",
     "shell.execute_reply": "2026-01-26T17:47:37.742783Z",
     "shell.execute_reply.started": "2026-01-26T17:47:37.573753Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting conversations...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b4d817cf44848d7af5547d833a46ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Format conversations\n",
    "print(\"Formatting conversations...\")\n",
    "formatted_dataset = dataset.map(\n",
    "    format_conversation,\n",
    "    remove_columns=dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T17:47:37.744888Z",
     "iopub.status.busy": "2026-01-26T17:47:37.744494Z",
     "iopub.status.idle": "2026-01-26T17:47:37.754064Z",
     "shell.execute_reply": "2026-01-26T17:47:37.753522Z",
     "shell.execute_reply.started": "2026-01-26T17:47:37.744862Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 1800\n",
      "Validation examples: 200\n"
     ]
    }
   ],
   "source": [
    "# Split into train/validation (90/10 split)\n",
    "split_dataset = formatted_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "print(f\"Training examples: {len(train_dataset)}\")\n",
    "print(f\"Validation examples: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T17:47:37.755105Z",
     "iopub.status.busy": "2026-01-26T17:47:37.754830Z",
     "iopub.status.idle": "2026-01-26T17:47:37.759803Z",
     "shell.execute_reply": "2026-01-26T17:47:37.758991Z",
     "shell.execute_reply.started": "2026-01-26T17:47:37.755074Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXAMPLE FORMATTED CONVERSATION:\n",
      "================================================================================\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Write a reflection on the complexities of diagnosing mental health disorders.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Diagnosing mental health disorders is a multifaceted and intricate process that demands a nuanced understanding of human psychology, behavior, and the socio-cultural factors that influence mental well-being. The complexity of this task is rooted in several factors that make the diagnosis of mental health d...\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview a formatted example\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXAMPLE FORMATTED CONVERSATION:\")\n",
    "print(\"=\"*80)\n",
    "print(train_dataset[0][\"text\"][:500] + \"...\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T17:47:37.761770Z",
     "iopub.status.busy": "2026-01-26T17:47:37.761443Z",
     "iopub.status.idle": "2026-01-26T17:47:39.574044Z",
     "shell.execute_reply": "2026-01-26T17:47:39.573487Z",
     "shell.execute_reply.started": "2026-01-26T17:47:37.761750Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c35946efcc5249f9904430a3d53d7132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d389a2aa2bc2421ab86ea74adc34435d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab7df25a36a942dea9d54d889536090d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.model_max_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T17:48:09.796140Z",
     "iopub.status.busy": "2026-01-26T17:48:09.795429Z",
     "iopub.status.idle": "2026-01-26T17:49:00.150324Z",
     "shell.execute_reply": "2026-01-26T17:49:00.149746Z",
     "shell.execute_reply.started": "2026-01-26T17:48:09.796111Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model in 4-bit...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5661ac50a6924a24999427cd69484690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "855d01974ad043669fa0f8433a166eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae1345c382e4414ad969a14df65746d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fce499303ca4671939934477ebcffb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc7b1a7a4a1f47fa9a1534a3cb0fdd23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b1c6ca0279440599dd6f341731df6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db68b4efd4a343279c0d8b5d0464563d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Loading model in 4-bit...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T17:49:08.927389Z",
     "iopub.status.busy": "2026-01-26T17:49:08.926788Z",
     "iopub.status.idle": "2026-01-26T17:49:13.113631Z",
     "shell.execute_reply": "2026-01-26T17:49:13.112930Z",
     "shell.execute_reply.started": "2026-01-26T17:49:08.927359Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T17:49:13.115068Z",
     "iopub.status.busy": "2026-01-26T17:49:13.114797Z",
     "iopub.status.idle": "2026-01-26T17:49:13.343958Z",
     "shell.execute_reply": "2026-01-26T17:49:13.343205Z",
     "shell.execute_reply.started": "2026-01-26T17:49:13.115046Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring LoRA...\n"
     ]
    }
   ],
   "source": [
    "print(\"Configuring LoRA...\")\n",
    "lora_config = LoraConfig(**LORA_CONFIG)\n",
    "\n",
    "# Add LoRA adapters\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T17:49:14.442478Z",
     "iopub.status.busy": "2026-01-26T17:49:14.441785Z",
     "iopub.status.idle": "2026-01-26T17:49:14.453522Z",
     "shell.execute_reply": "2026-01-26T17:49:14.452722Z",
     "shell.execute_reply.started": "2026-01-26T17:49:14.442449Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trainable parameters: 9,175,040 (0.51%)\n",
      "Total parameters: 1,812,638,720\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTrainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
    "print(f\"Total parameters: {total_params:,}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T17:49:19.575998Z",
     "iopub.status.busy": "2026-01-26T17:49:19.575484Z",
     "iopub.status.idle": "2026-01-26T17:49:22.848965Z",
     "shell.execute_reply": "2026-01-26T17:49:22.848445Z",
     "shell.execute_reply.started": "2026-01-26T17:49:19.575952Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up trainer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:2111: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ü§ó Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bc8f72b30b34132bb262b701bce7ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to train dataset:   0%|          | 0/1800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb8ec31e7f1d4923bfcd11fdf6b1d68f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/1800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "356321f7470c4247bda14d8bcdef4cea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/1800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (696 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c0e878d4ab241a8b7b87aa42a8806e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/1800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e15bbc855e5b4f249ae317971df219d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to eval dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c5ccc99617b4a019bd48955683e357a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9051d4281ebd4a7a9692acd17428f5d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a83e5b69fda45e9ad4a385874e66a60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Setting up trainer...\")\n",
    "training_args = TrainingArguments(**TRAINING_CONFIG)\n",
    "\n",
    "def formatting_func(example):\n",
    "    return example[\"text\"]\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    formatting_func=formatting_func\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T17:49:24.190933Z",
     "iopub.status.busy": "2026-01-26T17:49:24.190633Z",
     "iopub.status.idle": "2026-01-26T21:10:17.707169Z",
     "shell.execute_reply": "2026-01-26T21:10:17.706476Z",
     "shell.execute_reply.started": "2026-01-26T17:49:24.190907Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STARTING TRAINING\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='113' max='113' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [113/113 3:19:35, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.553400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.395900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.341800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.324000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.290200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.309900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.293300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.241900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.286400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.261100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.265600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=113, training_loss=1.3220192310029426, metrics={'train_runtime': 12053.0111, 'train_samples_per_second': 0.149, 'train_steps_per_second': 0.009, 'total_flos': 1.8356700584116224e+16, 'train_loss': 1.3220192310029426})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T18:44:02.441445Z",
     "iopub.status.busy": "2026-01-21T18:44:02.440687Z",
     "iopub.status.idle": "2026-01-21T18:44:03.032139Z",
     "shell.execute_reply": "2026-01-21T18:44:03.031198Z",
     "shell.execute_reply.started": "2026-01-21T18:44:02.441409Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model...\n",
      "\n",
      "‚úÖ Model saved to /kaggle/working/llama3.1-8b-ultramedical-lora\n",
      "\n",
      "Training complete! This is your Model_SFT (baseline for all experiments)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSaving model...\")\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"\\n‚úÖ Model saved to {OUTPUT_DIR}\")\n",
    "print(\"\\nTraining complete! This is your Model_SFT (baseline for all experiments)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T21:10:27.016510Z",
     "iopub.status.busy": "2026-01-26T21:10:27.016189Z",
     "iopub.status.idle": "2026-01-26T21:10:29.777687Z",
     "shell.execute_reply": "2026-01-26T21:10:29.776911Z",
     "shell.execute_reply.started": "2026-01-26T21:10:27.016481Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model...\n",
      "\n",
      "‚úÖ Model saved to /kaggle/working/Model_SFT_3B\n",
      "\n",
      "Zipping model...\n",
      "\n",
      "‚úÖ Model zipped at /kaggle/working/Model_SFT_3B.zip\n",
      "\n",
      "üéâ Training complete! This is your Model_SFT (baseline for all experiments)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "OUTPUT_DIR = \"/kaggle/working/Model_SFT_3B\"\n",
    "\n",
    "print(\"\\nSaving model...\")\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"\\n‚úÖ Model saved to {OUTPUT_DIR}\")\n",
    "\n",
    "# ---- ZIP THE MODEL ----\n",
    "zip_path = \"/kaggle/working/Model_SFT_3B.zip\"\n",
    "\n",
    "print(\"\\nZipping model...\")\n",
    "shutil.make_archive(\n",
    "    base_name=zip_path.replace(\".zip\", \"\"),\n",
    "    format=\"zip\",\n",
    "    root_dir=OUTPUT_DIR\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Model zipped at {zip_path}\")\n",
    "print(\"\\nüéâ Training complete! This is your Model_SFT (baseline for all experiments)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T21:11:00.900196Z",
     "iopub.status.busy": "2026-01-26T21:11:00.899890Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TESTING MODEL\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n",
      "Device set to use cuda:0\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "Caching is incompatible with gradient checkpointing in LlamaDecoderLayer. Setting `past_key_values=None`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test prompt: <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "A 55-year-old man presents with chest pain and shortness of breath. What are the key differential diagnoses to consider?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Model response:\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING MODEL\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Merge LoRA weights for inference (optional, for faster inference)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Create a text generation pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "# Test prompt\n",
    "test_prompt = \"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "A 55-year-old man presents with chest pain and shortness of breath. What are the key differential diagnoses to consider?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(\"Test prompt:\", test_prompt)\n",
    "print(\"\\nModel response:\")\n",
    "response = pipe(test_prompt)\n",
    "print(response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9294838,
     "sourceId": 14552359,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
